"""
Phase 3: Custom Knowledge Distillation (Response-based KD)

Hinton ë…¼ë¬¸ì˜ ì›ë˜ Knowledge Distillation êµ¬í˜„:
- Teacherì™€ Studentì˜ ìµœì¢… ì¶œë ¥(soft labels)ë§Œ ì‚¬ìš©
- ì¶”ë¡  ëª¨ë“œ ì¶œë ¥ ì‚¬ìš©ìœ¼ë¡œ ë™ì¼í•œ shape ë³´ì¥: [batch, 9, 8400]
- KL Divergenceë¡œ soft label ëª¨ë°©

Loss = Î± Ã— Hard_Loss + (1-Î±) Ã— TÂ² Ã— KL(student_soft || teacher_soft)
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from ultralytics.data.build import build_dataloader, build_yolo_dataset
from ultralytics.cfg import get_cfg
from ultralytics.utils import DEFAULT_CFG
import yaml
from tqdm import tqdm
import csv
from datetime import datetime

# Project paths
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_YAML = os.path.join(PROJECT_ROOT, "data", "data.yaml")
TEACHER_DIR = os.path.join(PROJECT_ROOT, "models", "teacher")
STUDENT_DIR = os.path.join(PROJECT_ROOT, "models", "student")


class ResponseKDLoss(nn.Module):
    """
    Response-based Knowledge Distillation Loss (Hinton et al., 2015)

    Teacherì™€ Studentì˜ ìµœì¢… ì¶œë ¥ì„ Temperatureë¡œ softení•˜ì—¬ ë¹„êµ
    """

    def __init__(self, temperature=4.0):
        super().__init__()
        self.T = temperature

    def forward(self, student_out, teacher_out):
        """
        Args:
            student_out: Student flattened ì¶œë ¥ [batch, features]
            teacher_out: Teacher flattened ì¶œë ¥ [batch, features]

        Returns:
            KL Divergence loss with temperature scaling
        """
        # ì´ë¯¸ flattenëœ ìƒíƒœë¡œ ë“¤ì–´ì˜´
        s_flat = student_out
        t_flat = teacher_out

        # Shape ë§ì¶”ê¸° (Teacherê°€ ë” í´ ìˆ˜ ìˆìŒ)
        if s_flat.shape[1] != t_flat.shape[1]:
            # Teacherë¥¼ Student í¬ê¸°ì— ë§ê²Œ ë³´ê°„
            t_flat = F.interpolate(
                t_flat.unsqueeze(1),
                size=s_flat.shape[1],
                mode='linear',
                align_corners=False
            ).squeeze(1)

        # Temperature scaling + Softmax
        s_soft = F.log_softmax(s_flat / self.T, dim=1)
        t_soft = F.softmax(t_flat / self.T, dim=1)

        # KL Divergence * T^2 (gradient magnitude ë³´ì •)
        loss = F.kl_div(s_soft, t_soft, reduction='batchmean') * (self.T ** 2)

        return loss


def get_dataloader(data_yaml, batch_size=8, img_size=640, mode='train'):
    """Create dataloader"""

    with open(data_yaml, 'r') as f:
        data_cfg = yaml.safe_load(f)

    if mode == 'train':
        img_path = os.path.join(PROJECT_ROOT, "data", "train", "images")
    else:
        img_path = os.path.join(PROJECT_ROOT, "data", "valid", "images")

    cfg = get_cfg(DEFAULT_CFG)
    cfg.data = data_yaml
    cfg.imgsz = img_size
    cfg.batch = batch_size

    dataset = build_yolo_dataset(
        cfg=cfg,
        img_path=img_path,
        batch=batch_size,
        data=data_cfg,
        mode=mode,
        rect=False,
        stride=32
    )

    dataloader = build_dataloader(
        dataset=dataset,
        batch=batch_size,
        workers=4,
        shuffle=(mode == 'train'),
        rank=-1
    )

    return dataloader


def train_with_kd(
    teacher_path,
    epochs=20,
    batch_size=8,
    img_size=640,
    temperature=4.0,
    alpha=0.5,
    lr=0.001,
    save_dir=None
):
    """
    Response-based Knowledge Distillation Training

    Hinton ë…¼ë¬¸ ë°©ì‹: Teacherì˜ soft labelì„ Studentê°€ ëª¨ë°©
    """

    # Device ì„¤ì •
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    print("\n" + "=" * 60)
    print("Response-based Knowledge Distillation (Hinton et al.)")
    print("=" * 60)

    print(f"\nì„¤ì •:")
    print(f"  - Teacher: {teacher_path}")
    print(f"  - Student: YOLOv8n (pretrained)")
    print(f"  - Temperature: {temperature}")
    print(f"  - Alpha: {alpha}")
    print(f"  - Epochs: {epochs}")
    print(f"  - Device: {device}")
    print(f"  - Loss: Î±Ã—Hard + (1-Î±)Ã—TÂ²Ã—KL(soft)")

    # =========================================================
    # 1. ëª¨ë¸ ë¡œë“œ
    # =========================================================
    print("\n[1/4] ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # Teacher (frozen, eval mode)
    teacher = YOLO(teacher_path)
    teacher_model = teacher.model.to(device)
    teacher_model.eval()
    for p in teacher_model.parameters():
        p.requires_grad = False
    print(f"  âœ… Teacher ë¡œë“œ: {sum(p.numel() for p in teacher_model.parameters()):,} params")

    # Student (trainable) - ê°™ì€ í´ë˜ìŠ¤ ìˆ˜(5)ë¡œ fine-tunedëœ ëª¨ë¸ ì‚¬ìš©
    # ê¸°ì¡´ fine-tuned student ëª¨ë¸ ê²½ë¡œë“¤
    student_paths = [
        os.path.join(STUDENT_DIR, "yolov8n_distilled", "weights", "best.pt"),
        os.path.join(STUDENT_DIR, "yolov8n_tank", "weights", "best.pt"),
    ]

    student_path = None
    for path in student_paths:
        if os.path.exists(path):
            student_path = path
            break

    if student_path:
        print(f"  ğŸ“¦ Fine-tuned Student ë¡œë“œ: {student_path}")
        student = YOLO(student_path)
    else:
        raise FileNotFoundError(
            "Fine-tuned Student ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. "
            "ë¨¼ì € 03_distillation.pyë¥¼ ì‹¤í–‰í•˜ì—¬ Studentë¥¼ 5 classesë¡œ í•™ìŠµí•˜ì„¸ìš”."
        )

    student_model = student.model.to(device)
    print(f"  âœ… Student ë¡œë“œ: {sum(p.numel() for p in student_model.parameters()):,} params")
    print(f"  âœ… Student nc: {student_model.model[-1].nc}")

    # =========================================================
    # 2. í•™ìŠµ ì„¤ì •
    # =========================================================
    print("\n[2/4] í•™ìŠµ ì„¤ì •...")

    # KD Loss
    kd_loss_fn = ResponseKDLoss(temperature=temperature)

    # Optimizer
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=lr, weight_decay=0.0005)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr*0.01)

    # Save directory
    if save_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = os.path.join(STUDENT_DIR, f"yolov8n_kd_{timestamp}")
    os.makedirs(os.path.join(save_dir, "weights"), exist_ok=True)
    print(f"  âœ… ì €ì¥ ê²½ë¡œ: {save_dir}")

    # =========================================================
    # 3. ë°ì´í„° ë¡œë”
    # =========================================================
    print("\n[3/4] ë°ì´í„° ë¡œë” ìƒì„±...")
    train_loader = get_dataloader(DATA_YAML, batch_size, img_size, 'train')
    print(f"  âœ… Train ë°°ì¹˜: {len(train_loader)}")

    # =========================================================
    # 4. í•™ìŠµ ë£¨í”„
    # =========================================================
    print("\n[4/4] Knowledge Distillation í•™ìŠµ...")
    print(f"\n  ì¶”ë¡  ëª¨ë“œ ì¶œë ¥ ì‚¬ìš©: [batch, 9, 8400]")
    print(f"  Teacherì™€ Student ì¶œë ¥ shape ë™ì¼")

    history = []
    best_loss = float('inf')

    for epoch in range(epochs):
        student_model.train()
        epoch_loss = 0
        epoch_kd_loss = 0
        num_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")

        for batch in pbar:
            images = batch['img'].to(device).float() / 255.0

            # ============================================
            # Teacher forward (í•™ìŠµ ëª¨ë“œ ì¶œë ¥ ì‚¬ìš©)
            # ============================================
            # YOLOv8 í•™ìŠµ ëª¨ë“œ: 3ê°œ ìŠ¤ì¼€ì¼ì˜ feature maps ë°˜í™˜
            # ê° feature mapì„ flattení•˜ì—¬ ì‚¬ìš©
            with torch.no_grad():
                teacher_model.eval()
                t_out = teacher_model.model(images)  # ë‚´ë¶€ model ì§ì ‘ í˜¸ì¶œ
                # í•™ìŠµ ëª¨ë“œ ì¶œë ¥: list of tensors
                if isinstance(t_out, (list, tuple)):
                    # ëª¨ë“  ì¶œë ¥ì„ concatí•˜ì—¬ í•˜ë‚˜ì˜ tensorë¡œ
                    teacher_out = torch.cat([t.flatten(1) for t in t_out], dim=1)
                else:
                    teacher_out = t_out.flatten(1)

            # ============================================
            # Student forward (í•™ìŠµ ëª¨ë“œ, gradient ìœ ì§€)
            # ============================================
            student_model.train()
            s_out = student_model.model(images)  # ë‚´ë¶€ model ì§ì ‘ í˜¸ì¶œ
            if isinstance(s_out, (list, tuple)):
                student_out = torch.cat([s.flatten(1) for s in s_out], dim=1)
            else:
                student_out = s_out.flatten(1)

            # Shape í™•ì¸ (ì²« ë°°ì¹˜ë§Œ)
            if epoch == 0 and num_batches == 0:
                print(f"\n  Teacher output shape: {teacher_out.shape}")
                print(f"  Student output shape: {student_out.shape}")

            # ============================================
            # KD Loss ê³„ì‚°
            # ============================================
            kd_loss = kd_loss_fn(student_out, teacher_out)

            # Total loss (í˜„ì¬ëŠ” KD lossë§Œ, hard loss ì¶”ê°€ ê°€ëŠ¥)
            total_loss = kd_loss

            # Backward
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # Metrics
            epoch_loss += total_loss.item()
            epoch_kd_loss += kd_loss.item()
            num_batches += 1

            pbar.set_postfix({
                'loss': f'{total_loss.item():.4f}',
                'kd': f'{kd_loss.item():.4f}'
            })

        scheduler.step()

        # Epoch summary
        avg_loss = epoch_loss / num_batches
        avg_kd = epoch_kd_loss / num_batches

        print(f"\n  Epoch {epoch+1}: Loss={avg_loss:.4f}, KD={avg_kd:.4f}, LR={scheduler.get_last_lr()[0]:.6f}")

        history.append({
            'epoch': epoch + 1,
            'loss': avg_loss,
            'kd_loss': avg_kd,
            'lr': scheduler.get_last_lr()[0]
        })

        # Save best
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(student_model.state_dict(), os.path.join(save_dir, "weights", "best.pt"))
            print(f"  âœ… Best model saved! (loss: {best_loss:.4f})")

        # Save last
        torch.save(student_model.state_dict(), os.path.join(save_dir, "weights", "last.pt"))

    # Save history
    csv_path = os.path.join(save_dir, "results.csv")
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['epoch', 'loss', 'kd_loss', 'lr'])
        writer.writeheader()
        writer.writerows(history)

    print("\n" + "=" * 60)
    print("Knowledge Distillation ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nBest model: {os.path.join(save_dir, 'weights', 'best.pt')}")
    print(f"Best loss: {best_loss:.4f}")

    # Convert to Ultralytics format for evaluation
    print("\nëª¨ë¸ ë³€í™˜ ì¤‘...")
    try:
        student_eval = YOLO("yolov8n.pt")
        student_eval.model.load_state_dict(
            torch.load(os.path.join(save_dir, "weights", "best.pt"), map_location=device)
        )
        # Save as YOLO format
        save_path = os.path.join(save_dir, "weights", "best_yolo.pt")
        torch.save({
            'model': student_eval.model,
            'train_args': {'imgsz': img_size, 'nc': 5}
        }, save_path)
        print(f"  âœ… YOLO í˜•ì‹ ì €ì¥: {save_path}")
    except Exception as e:
        print(f"  âš ï¸ ë³€í™˜ ì‹¤íŒ¨: {e}")

    return save_dir


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Response-based Knowledge Distillation")
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--batch", type=int, default=8)
    parser.add_argument("--temperature", type=float, default=4.0)
    parser.add_argument("--alpha", type=float, default=0.5)
    parser.add_argument("--lr", type=float, default=0.001)

    args = parser.parse_args()

    teacher_path = os.path.join(TEACHER_DIR, "yolov8m_tank", "weights", "best.pt")

    if not os.path.exists(teacher_path):
        print(f"âŒ Teacher ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤: {teacher_path}")
        exit(1)

    train_with_kd(
        teacher_path=teacher_path,
        epochs=args.epochs,
        batch_size=args.batch,
        temperature=args.temperature,
        alpha=args.alpha,
        lr=args.lr
    )
