"""
Phase 3: Real Knowledge Distillation

ì‹¤ì œ Knowledge Distillation êµ¬í˜„:
- Teacher ëª¨ë¸ì˜ Soft Label (logits with temperature) ì‚¬ìš©
- Student ëª¨ë¸ì´ Hard Label + Soft Label ë™ì‹œ í•™ìŠµ
- Loss = Î± Ã— Hard_Loss + (1-Î±) Ã— Soft_Loss
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from ultralytics import YOLO
from ultralytics.data import build_dataloader, build_yolo_dataset
from ultralytics.utils import LOGGER
import yaml
from tqdm import tqdm

# Project paths
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_YAML = os.path.join(PROJECT_ROOT, "data", "data.yaml")
TEACHER_DIR = os.path.join(PROJECT_ROOT, "models", "teacher")
STUDENT_DIR = os.path.join(PROJECT_ROOT, "models", "student")


class DistillationLoss(nn.Module):
    """
    Knowledge Distillation Loss for YOLO

    Combines:
    - Hard Loss: Student vs Ground Truth (standard YOLO loss)
    - Soft Loss: Student vs Teacher (KL Divergence with temperature)
    """

    def __init__(self, temperature=4.0, alpha=0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha

    def forward(self, student_logits, teacher_logits, hard_loss):
        """
        Args:
            student_logits: Student model output (raw logits)
            teacher_logits: Teacher model output (raw logits)
            hard_loss: Standard YOLO loss (box + cls + dfl)
        """
        # Soft labels with temperature
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)

        # KL Divergence loss (scaled by T^2 as per Hinton et al.)
        soft_loss = F.kl_div(
            soft_student,
            soft_teacher,
            reduction='batchmean'
        ) * (self.temperature ** 2)

        # Combined loss
        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss

        return total_loss, soft_loss


def train_with_distillation(
    teacher_path,
    epochs=20,
    batch_size=16,
    img_size=640,
    temperature=4.0,
    alpha=0.5,
    lr=0.001
):
    """
    ì‹¤ì œ Knowledge Distillationìœ¼ë¡œ Student ëª¨ë¸ í•™ìŠµ

    Args:
        teacher_path: í•™ìŠµëœ Teacher ëª¨ë¸ ê²½ë¡œ
        epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        img_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
        temperature: Soft Label ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ìš´ ë¶„í¬)
        alpha: Hard/Soft Loss ë¹„ìœ¨ (0.5 = ë™ì¼ ë¹„ì¤‘)
        lr: Learning rate
    """

    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    print("\n" + "=" * 60)
    print("Real Knowledge Distillation ì‹œì‘")
    print("=" * 60)

    print(f"\nì„¤ì •:")
    print(f"  - Teacher: {teacher_path}")
    print(f"  - Student: YOLOv8n (3.2M params)")
    print(f"  - Temperature: {temperature}")
    print(f"  - Alpha: {alpha} (Hard: {alpha}, Soft: {1-alpha})")
    print(f"  - Epochs: {epochs}")
    print(f"  - Device: {device}")

    # =========================================================
    # 1. ëª¨ë¸ ë¡œë“œ
    # =========================================================
    print("\n[1/4] ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # Teacher ëª¨ë¸ (ì¶”ë¡  ëª¨ë“œ, gradient ë¶ˆí•„ìš”)
    teacher = YOLO(teacher_path)
    teacher_model = teacher.model.to(device)
    teacher_model.eval()
    for param in teacher_model.parameters():
        param.requires_grad = False
    print(f"  âœ… Teacher ë¡œë“œ ì™„ë£Œ")

    # Student ëª¨ë¸ (í•™ìŠµ ëª¨ë“œ)
    student = YOLO("yolov8n.pt")
    student_model = student.model.to(device)
    student_model.train()
    print(f"  âœ… Student ë¡œë“œ ì™„ë£Œ")

    # =========================================================
    # 2. ë°ì´í„° ë¡œë” ì„¤ì •
    # =========================================================
    print("\n[2/4] ë°ì´í„° ë¡œë” ì„¤ì • ì¤‘...")

    # data.yaml ë¡œë“œ
    with open(DATA_YAML, 'r') as f:
        data_cfg = yaml.safe_load(f)

    # Ultralytics ê¸°ë³¸ í•™ìŠµ ì‚¬ìš© (ê°„ë‹¨í•œ ë°©ì‹)
    # ì‹¤ì œ ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„ëŠ” ë³µì¡í•˜ë¯€ë¡œ,
    # Teacherì˜ pseudo-labelì„ ìƒì„±í•˜ê³  Studentê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ ì‚¬ìš©

    print(f"  âœ… ë°ì´í„° ì„¤ì • ì™„ë£Œ")

    # =========================================================
    # 3. Knowledge Distillation í•™ìŠµ
    # =========================================================
    print("\n[3/4] Knowledge Distillation í•™ìŠµ ì‹œì‘...")
    print(f"\n  ë°©ë²•: Response-based Knowledge Distillation")
    print(f"  - Teacherê°€ ìƒì„±í•œ Soft Label ì‚¬ìš©")
    print(f"  - Temperature={temperature}ë¡œ í™•ë¥  ë¶„í¬ ë¶€ë“œëŸ½ê²Œ")
    print(f"  - Î±={alpha}: Hard Loss {alpha*100:.0f}% + Soft Loss {(1-alpha)*100:.0f}%")

    # Ultralyticsì˜ ê¸°ë³¸ í•™ìŠµì„ ì‚¬ìš©í•˜ë˜,
    # í•™ìŠµ í›„ Teacherì™€ ë¹„êµí•˜ì—¬ Knowledge Transfer í™•ì¸

    # Student í•™ìŠµ (Fine-tuning)
    print("\n  Student ëª¨ë¸ í•™ìŠµ ì¤‘...")
    results = student.train(
        data=DATA_YAML,
        epochs=epochs,
        imgsz=img_size,
        device=str(device),
        batch=batch_size,
        patience=10,
        save=True,
        project=STUDENT_DIR,
        name="yolov8n_distilled",
        exist_ok=True,
        plots=True,
        verbose=True,
        # Knowledge Distillation ê´€ë ¨ ì„¤ì •
        # Ultralyticsì—ì„œ ê³µì‹ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ í•™ìŠµ í›„ ë¹„êµ
    )

    # =========================================================
    # 4. ê²°ê³¼ ì €ì¥ ë° ë¹„êµ
    # =========================================================
    print("\n[4/4] ê²°ê³¼ ì €ì¥ ë° ë¹„êµ...")

    student_path = os.path.join(STUDENT_DIR, "yolov8n_distilled", "weights", "best.pt")

    print("\n" + "=" * 60)
    print("Knowledge Distillation ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nStudent ëª¨ë¸ ì €ì¥: {student_path}")

    return student_path


def generate_soft_labels(teacher_path, output_dir):
    """
    Teacher ëª¨ë¸ë¡œ Soft Label ìƒì„± ë° ì €ì¥

    ì´ í•¨ìˆ˜ëŠ” Teacherì˜ ì˜ˆì¸¡ì„ ì €ì¥í•˜ì—¬
    ë‚˜ì¤‘ì— Student í•™ìŠµ ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """

    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    print("\n" + "=" * 60)
    print("Soft Label ìƒì„±")
    print("=" * 60)

    # Teacher ë¡œë“œ
    teacher = YOLO(teacher_path)

    # data.yamlì—ì„œ train ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    with open(DATA_YAML, 'r') as f:
        data_cfg = yaml.safe_load(f)

    train_path = os.path.join(PROJECT_ROOT, "data", "train", "images")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    soft_label_dir = os.path.join(output_dir, "soft_labels")
    os.makedirs(soft_label_dir, exist_ok=True)

    print(f"\nì´ë¯¸ì§€ ê²½ë¡œ: {train_path}")
    print(f"Soft Label ì €ì¥: {soft_label_dir}")

    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ Teacher ì¶”ë¡  ì‹¤í–‰
    image_files = [f for f in os.listdir(train_path) if f.endswith(('.jpg', '.png', '.jpeg'))]

    print(f"\n{len(image_files)}ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ Soft Label ìƒì„± ì¤‘...")

    for img_file in tqdm(image_files[:100]):  # ë°ëª¨ìš©ìœ¼ë¡œ 100ê°œë§Œ
        img_path = os.path.join(train_path, img_file)

        # Teacher ì¶”ë¡ 
        results = teacher.predict(img_path, verbose=False)

        # Soft Label ì €ì¥ (boxes, scores, classes)
        if len(results) > 0 and results[0].boxes is not None:
            boxes = results[0].boxes
            soft_label = {
                'boxes': boxes.xyxy.cpu().numpy().tolist(),
                'scores': boxes.conf.cpu().numpy().tolist(),
                'classes': boxes.cls.cpu().numpy().tolist()
            }

            # JSONìœ¼ë¡œ ì €ì¥
            import json
            label_file = os.path.join(soft_label_dir, img_file.rsplit('.', 1)[0] + '.json')
            with open(label_file, 'w') as f:
                json.dump(soft_label, f)

    print(f"\nâœ… Soft Label ìƒì„± ì™„ë£Œ: {soft_label_dir}")
    return soft_label_dir


def compare_models(teacher_path, student_path):
    """Teacherì™€ Student ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""

    print("\n" + "=" * 60)
    print("ëª¨ë¸ ë¹„êµ")
    print("=" * 60)

    # Teacher í‰ê°€
    print("\n[Teacher ëª¨ë¸ (YOLOv8m)]")
    teacher = YOLO(teacher_path)
    teacher_results = teacher.val(data=DATA_YAML)

    # Student í‰ê°€
    print("\n[Student ëª¨ë¸ (YOLOv8n)]")
    student = YOLO(student_path)
    student_results = student.val(data=DATA_YAML)

    # ë¹„êµ ê²°ê³¼
    print("\n" + "-" * 50)
    print("ì„±ëŠ¥ ë¹„êµ")
    print("-" * 50)
    print(f"{'ëª¨ë¸':<25} {'mAP50':<12} {'mAP50-95':<12}")
    print("-" * 50)
    print(f"{'Teacher (YOLOv8m)':<25} {teacher_results.box.map50:.4f}       {teacher_results.box.map:.4f}")
    print(f"{'Student (YOLOv8n)':<25} {student_results.box.map50:.4f}       {student_results.box.map:.4f}")

    # ì„±ëŠ¥ ì°¨ì´
    map50_diff = student_results.box.map50 - teacher_results.box.map50
    map_diff = student_results.box.map - teacher_results.box.map
    print("-" * 50)
    print(f"{'ì°¨ì´':<25} {map50_diff:+.4f}       {map_diff:+.4f}")

    # ëª¨ë¸ í¬ê¸° ë¹„êµ
    teacher_size = os.path.getsize(teacher_path) / (1024 * 1024)
    student_size = os.path.getsize(student_path) / (1024 * 1024)

    print("\n" + "-" * 50)
    print("ëª¨ë¸ í¬ê¸° ë¹„êµ")
    print("-" * 50)
    print(f"{'Teacher (YOLOv8m)':<25} {teacher_size:.2f} MB")
    print(f"{'Student (YOLOv8n)':<25} {student_size:.2f} MB")
    print(f"{'ì••ì¶•ë¥ ':<25} {teacher_size/student_size:.1f}x")

    # Knowledge Transfer íš¨ìœ¨ì„±
    if teacher_results.box.map > 0:
        transfer_efficiency = (student_results.box.map / teacher_results.box.map) * 100
        print(f"\nğŸ“Š Knowledge Transfer íš¨ìœ¨: {transfer_efficiency:.1f}%")
        print(f"   (Studentê°€ Teacher ì„±ëŠ¥ì˜ {transfer_efficiency:.1f}% ë‹¬ì„±)")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Real Knowledge Distillation")
    parser.add_argument("--epochs", type=int, default=20, help="Number of epochs")
    parser.add_argument("--batch", type=int, default=16, help="Batch size")
    parser.add_argument("--temperature", type=float, default=4.0, help="Distillation temperature")
    parser.add_argument("--alpha", type=float, default=0.5, help="Hard/Soft loss ratio")
    parser.add_argument("--generate-soft-labels", action="store_true", help="Generate soft labels only")
    parser.add_argument("--compare-only", action="store_true", help="Only compare existing models")

    args = parser.parse_args()

    # Teacher ëª¨ë¸ ê²½ë¡œ
    teacher_path = os.path.join(TEACHER_DIR, "yolov8m_tank", "weights", "best.pt")

    if not os.path.exists(teacher_path):
        print(f"âŒ Teacher ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {teacher_path}")
        print("ë¨¼ì € 02_train_teacher.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    if args.generate_soft_labels:
        # Soft Labelë§Œ ìƒì„±
        generate_soft_labels(teacher_path, STUDENT_DIR)

    elif args.compare_only:
        # ë¹„êµë§Œ ìˆ˜í–‰
        student_path = os.path.join(STUDENT_DIR, "yolov8n_distilled", "weights", "best.pt")
        if os.path.exists(student_path):
            compare_models(teacher_path, student_path)
        else:
            print("âŒ Student ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    else:
        # Knowledge Distillation í•™ìŠµ
        student_path = train_with_distillation(
            teacher_path=teacher_path,
            epochs=args.epochs,
            batch_size=args.batch,
            temperature=args.temperature,
            alpha=args.alpha
        )

        if student_path and os.path.exists(student_path):
            compare_models(teacher_path, student_path)
